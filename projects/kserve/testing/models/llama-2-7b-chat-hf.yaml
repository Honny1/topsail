extends: base

serving_runtime:
  transformer:
    resource_request:
      cpu: 2
      memory: 4 # in Gi
    extra_env:
      RUNTIME_GRPC_SERVER_THREAD_POOL_SIZE: 160
  kserve:
    resource_request:
      cpu: 4
      memory: 24 # in Gi
      nvidia.com/gpu: 1
      nvidia.com/gpu_memory: 40 # in Gi of GPU memory
    extra_env: {}
inference_service:
  storage_uri: "s3://psap-hf-models/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf"
  model_format: pytorch
  single_container: true
