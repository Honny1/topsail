serving_runtime:
  kserve:
    resource_request:
      cpu: 8
    extra_env:
      TRANSFORMERS_CACHE: "/tmp/transformers_cache"
inference_service:
  model_format: "pytorch"
  min_replicas: 1
  maxReplicas: 1
  single_container: true