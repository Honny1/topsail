__platforms: &all_platforms
  - macos/llama_cpp/metal
  - macos/llama_cpp/vulkan
  - macos/llama_cpp/upstream_bin
  - podman/llama_cpp/vulkan
  - macos/ollama
  - macos/ramalama

ci_presets:
  # list of names of presets to apply, or a single name, or null if not preset
  to_apply: [multi-users, llama-cpp]

  # dict of variables to apply
  variable_overrides: {}

  # list of names of presets that have been applied
  names: []

  local_config: null # defined locally in variable_overrides.yaml

  mac4:
    secrets.hostname: mac_ai__hostname.mac4
    secrets.username: mac_ai__username.mac4
    secrets.base_work_dir: mac_ai__base_work_dir.mac4

  mac5:
    secrets.hostname: mac_ai__hostname.mac5
    secrets.username: mac_ai__username.mac5
    secrets.base_work_dir: mac_ai__base_work_dir.mac5

  mac-m4:
    secrets.hostname: mac_ai__hostname.mac-m4
    secrets.username: mac_ai__username.mac-m4
    secrets.base_work_dir: mac_ai__base_work_dir.mac-m4

  # ---

  multi-servers:
    test.platform:
    - podman/ramalama
    - podman/llama_cpp/vulkan
    - macos/ollama
    - macos/llama_cpp/upstream_bin
    prepare.platforms.to_build: null # use test.platform

  # ---

  benchmark:
    test.matbenchmarking.enabled: true
    test.platform: *all_platforms
    test.model.name: llama3.2

    test.llm_load_test.args.concurrency: 1
    test.llm_load_test.args.duration: 300

    prepare.brew.capture_dependencies: true

  bench_only:
    test.llm_load_test.enabled: false
    test.inference_server.benchmark.enabled: true

  inference_only:
    test.llm_load_test.enabled: true
    test.inference_server.benchmark.enabled: false

  multi-version:
    prepare.llama_cpp.repo.version:
    - b4688 # slp's
    - b4897 # latest - 25-03-17

  multi-model:
    test.model.name:
    - Mistral-7B-Instruct-v0.2-GGUF
    - llama2
    - llama3.2

  multi-quant:
    test.model.name:
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_S
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_0
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q3_K_M
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q5_K_S
    prepare.platforms.to_build: null # use test.platform
    test.platform:
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan

  multi-users:
    test.matbenchmarking.enabled: true
    test.llm_load_test.args.concurrency: [1, 2, 4]

  # ---

  llama-cpp:
    prepare.platforms.to_build: null # use test.platform
    test.platform:
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan
    - podman/llama_cpp/vulkan

  keep_running:
    test.inference_server.unload_on_exit: false
    prepare.podman.stop_on_exit: false

secrets:
  dir:
    name: crc-mac-ai-secret
    env_key: CRC_MAC_AI_SECRET_PATH
  private_key_path: mac_ai__private_key
  hostname: mac_ai__hostname.mac-m4
  username: mac_ai__username.mac-m4
  base_work_dir: mac_ai__base_work_dir.mac-m4

remote_host:
  run_locally: false
  private_key_filename: "@secrets.private_key_path" # in the secret dir
  hostname: "*$@secrets.hostname"
  username: "*$@secrets.username"
  port: 22
  base_work_dir: "*$@secrets.base_work_dir"
  ssh_flags:
  - -oStrictHostKeyChecking=no
  - -oUserKnownHostsFile=/dev/null
  - -o LogLevel=ERROR
  system: darwin
  arch: arm64
  python_bin: python3
  podman_bin: podman # only used if not prepare.podman.repo.enabled
  env:
    PATH: "/opt/homebrew/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin"
  home_is_base_work_dir: true
  verbose_ssh_commands: true

prepare:
  # the list of platform environments to prepare
  platforms:
    to_build: *all_platforms

    model_pullers:
      llama_cpp: macos/llama_cpp/upstream_bin
      ollama: macos/ollama
      ramalama: podman/ramalama

  cleanup_on_exit: false
  prepare_only_inference_server: false

  llm_load_test:
    install_requirements: false

  llama_cpp:
    iterable_build_fields:
    - prepare.llama_cpp.repo.version
    - prepare.llama_cpp.repo.source.cmake.parallel
    repo:
      url: https://github.com/ggml-org/llama.cpp
      version: b4897
      source:
        cmake:
          parallel: 5
          openmp:
            enabled: true
            flags: -DGGML_OPENMP=ON -DOpenMP_C_FLAGS="-I{@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/include/" -DOpenMP_C_LIB_NAMES=omp -DOpenMP_omp_LIBRARY={@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/lib/libomp.dylib -DOpenMP_CXX_FLAGS="-I{@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/include/" -DOpenMP_CXX_LIB_NAMES=omp
          common: -DGGML_CPU_ARM_ARCH=native
          flavors:
            metal: -DGGML_NATIVE=ON -DGGML_VULKAN=OFF
            vulkan: -DGGML_VULKAN=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF -DVulkan_INCLUDE_DIR=/opt/homebrew/include/ -DVulkan_LIBRARY=/opt/homebrew/lib/libMoltenVK.dylib
      darwin:
        upstream_bin:
          file: llama-{@prepare.llama_cpp.repo.version}-bin-macos-{@remote_host.arch}.zip
          tarball: true
      podman:
        build_from: local_container_file # desktop_playground
        command: # will be set in prepare_test

        local_container_file:
          path: images/llama_cpp.containerfile
          command: /app/llama.cpp/build/bin/llama-server
          build_args:
            LLAMA_CPP_VERSION: # set at runtime from prepare.llama_cpp.repo.version
            LLAMA_CPP_CMAKE_BUILD_FLAGS: # set at runtime with --parallel
            # ..flavors.<name> will be appended to this
            LLAMA_CPP_CMAKE_FLAGS:
          flavors:
            kompute: -DGGML_KOMPUTE=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF
            vulkan: -DGGML_VULKAN=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF
        desktop_playground:
          url: https://github.com/slp/podman-desktop-extension-ai-lab-playground-images
          # url: https://github.com/containers/podman-desktop-extension-ai-lab-playground-images
          ref: 188866acbdd7efcdd438f21c09139ae378820a00
          root_directory: chat
          prepare_script: setup.sh
          container_file: vulkan/arm64/Containerfile
          image: llama_cpp
          tag: # use the commit first chars
          command: env PYTHONPATH=/locallm python -m llama_cpp.server

  ollama:
    repo:
      url: https://github.com/ollama/ollama/
      version: v0.5.7
      macos:
        file: ollama-darwin

  ramalama:
    repo:
      url: https://github.com/containers/ramalama
      version: v0.6.4

  brew:
    install_dependencies: false
    capture_dependencies: false
    _libomp:
      location: /opt/homebrew/Cellar/libomp/
      version: "20.1.0"
    dependencies:
    # brew tap slp/krunkit
    - krunkit
    # brew link --overwrite molten-vk-krunkit
    - molten-vk-krunkit

    - vulkan-headers
    - libkrun-efi
    - virglrenderer
    - molten-vk
    - cmake
    - shaderc
    - libomp
    - glslang

  podman:
    repo:
      enabled: true
      url: https://github.com/containers/podman
      version: v5.4.0
      darwin:
        file: podman-remote-release-{@remote_host.system}_{@remote_host.arch}.zip
        zip: true
    gvisor:
      repo:
        enabled: true
        url: https://github.com/containers/gvisor-tap-vsock/
        version: v0.8.3
        file: "gvproxy-{@remote_host.system}"

    container:
      name: topsail_mac_ai

      image: quay.io/slopezpa/fedora-vgpu:latest # arm64 image
      python_bin: python3
      system: linux
      device: /dev/dri

    stop_on_exit: true

    machine:
      enabled: true
      force_configuration: false
      set_default: true
      name: podman-machine-default
      configuration:
        cpus: 4 # in cores
        memory: 5120 # in MiB
      env:
        CONTAINERS_MACHINE_PROVIDER: libkrun
        CONTAINERS_HELPER_BINARY_DIR: /opt/homebrew/bin/

test:
  platform: *all_platforms

  platforms_to_skip:
  # - macos/llama_cpp/upstream_bin # always built, but no need to test it every time
  - podman/llama_cpp/no-gpu/kompute # slow, not interesting atm

  inference_server:
    port: 11434
    unload_on_exit: true
    stop_on_exit: true

    benchmark:
      enabled: true
      llama_cpp:
        llama_bench: true
        backend_ops_perf: false
  model:
    name: llama3.2
    size: small
    gguf_dir: models

  llm_load_test:
    enabled: true

    args:
      host: localhost
      port: "@test.inference_server.port"
      duration: 60
      concurrency: 1
      plugin: openai_plugin
      interface: http
      streaming: true
      endpoint: "/v1/chat/completions"

    dataset_sizes:
      small:
        max_input_tokens: 2047
        max_output_tokens: 1024
        max_sequence_tokens: 2048
      large:
        max_input_tokens: 3500
        max_output_tokens: 800
        max_sequence_tokens: 3500
  matbenchmarking:
    enabled: false
    stop_on_error: true
    fields:
    - test.model.name
    - test.platform
    - test.llm_load_test.args
    - prepare.llama_cpp.repo.version
  capture_metrics:
    enabled: true
    gpu:
      # needs this in visudo:
      # $USER ALL = (root) NOPASSWD: /usr/bin/pkill powermetrics
      # $USER ALL = (root) NOPASSWD: /usr/bin/powermetrics *
      enabled: true
      sampler: gpu_power
      rate: 1000
    virtgpu:
      enabled: true

cleanup:
  models:
    gguf: true
    ollama: true
    ramalama: true

  files:
    llama_cpp: true
    ollama: true
    ramalama: true
    llm-load-test: true
    podman: true

  images:
    llama_cpp: true

  podman_machine:
    delete: true
    reset: true

export_artifacts:
  enabled: false

matbench:
  enabled: true
  preset: null
  workload: projects.mac_ai.visualizations.llm_load_test
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  # directory to plot. Set by topsail/testing/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: false

exec_list:
  _only_: false

  pre_cleanup_ci: null
  prepare_ci: null
  test_ci: null
  post_cleanup_ci: null
  matbench_run: true
  generate_plots_from_pr_args: true

__platform_check:
  system: [macos, podman]
  inference_server: [llama_cpp, ollama, ramalama]

  options:
    no_gpu: no-gpu

  flavors:
    llama_cpp: [metal, vulkan, upstream_bin, kompute]
