ci_presets:
  # list of names of presets to apply, or a single name, or null if not preset
  to_apply: []

  # dict of variables to apply
  variable_overrides: {}

  # list of names of presets that have been applied
  names: []

  local_config: null # defined locally in variable_overrides.yaml

  benchmark:
    test.matbenchmarking.enabled: true
    test.platform: [native, podman]

    test.llm_load_test.matbenchmarking: true
    test.llm_load_test.args.concurrency: [1, 2, 4]
    test.llm_load_test.args.duration: 300
    test.model.name: llama3.2

secrets:
  dir:
    name: pod-virt-secret
    env_key: POD_VIRT_SECRET_PATH
  private_key_path: host__private_key
  hostname: host__hostname
  hostport: host__hostport
  base_work_dir: host__base_work_dir


remote_host:
  run_locally: false
  private_key_path: "@secrets.private_key_path"
  name: "@secrets.hostname"
  port: "@secrets.hostport"
  base_work_dir: "@secrets.base_work_dir"
  ssh_flags:
  - -oStrictHostKeyChecking=no
  - -oUserKnownHostsFile=/dev/null
  - -o LogLevel=ERROR
  system: darwin
  arch: arm64
  python_bin: /opt/homebrew/bin/python3
  podman_bin: /opt/podman/bin/podman

prepare:
  cleanup_on_exit: false
  ollama:
    repo:
      url: https://github.com/ollama/ollama/
      version: v0.5.7
      darwin:
        file: ollama-darwin
        executable: true
      linux:
        file: ollama-linux-{@remote_host.arch}.tgz
        tarball: true
    port: 11434
    unload_on_exit: true
    stop_on_exit: true

  # the list of system binaries to prepare
  systems: [darwin, linux]

  podman:
    container:
      name: topsail_mac_ai

      image: quay.io/slopezpa/fedora-vgpu:latest # arm64 image
      python_bin: python3
      system: linux
      device: /dev/dri

    stop_on_exit: true

    machine:
      enabled: true
      set_default: true
      name: podman-machine-default
      configuration:
        cpus: 4 # in cores
        memory: 5000 # in MiB
      env:
        CONTAINERS_MACHINE_PROVIDER: libkrun
        CONTAINERS_HELPER_BINARY_DIR: /opt/homebrew/bin/

test:
  platform: podman

  model:
    name: smollm:135m
    size: small

  llm_load_test:
    enabled: true
    matbenchmarking: false

    args:
      host: localhost
      port: "@prepare.ollama.port"
      duration: 30
      concurrency: 1
      plugin: openai_plugin
      interface: http
      streaming: true
      endpoint: "/v1/chat/completions"

    dataset_sizes:
      small:
        max_input_tokens: 2047
        max_output_tokens: 1024
        max_sequence_tokens: 2048
      large:
        max_input_tokens: 3500
        max_output_tokens: 800
        max_sequence_tokens: 3500
  matbenchmarking:
    enabled: false
    stop_on_error: true

export_artifacts:
  enabled: false

matbench:
  enabled: true
  preset: null
  workload: projects.mac_ai.visualizations.llm_load_test
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  # directory to plot. Set by topsail/testing/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: false

exec_list:
  _only_: false

  pre_cleanup_ci: null
  prepare_ci: null
  test_ci: null
  post_cleanup_ci: null
  matbench_run_with_deploy: true
  matbench_run_without_deploy: true
