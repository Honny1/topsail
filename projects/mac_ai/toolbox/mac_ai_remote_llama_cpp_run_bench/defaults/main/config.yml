# Auto-generated file, do not edit manually ...
# Toolbox generate command: repo generate_ansible_default_settings
# Source component: Mac_Ai.remote_llama_cpp_run_bench

# Parameters
# the path to the llama.cpp bin directory
# Mandatory value
mac_ai_remote_llama_cpp_run_bench_path:

# the name of the model to use
# Mandatory value
mac_ai_remote_llama_cpp_run_bench_model_name:

# the prefix to get the llama.cpp binaries running
mac_ai_remote_llama_cpp_run_bench_prefix:

# number of layers to store in VRAM
mac_ai_remote_llama_cpp_run_bench_ngl: 99

# if true, runs the benchmark in verbose mode
mac_ai_remote_llama_cpp_run_bench_verbose: true

# if true, runs the llama-bench benchmark
mac_ai_remote_llama_cpp_run_bench_llama_bench: true

# if true, runs the test-backend-ops benchmark
mac_ai_remote_llama_cpp_run_bench_test_backend_ops: true

# Default Ansible variables
# Default value for ansible_os_family to ensure role remains standalone
ansible_os_family: Linux
