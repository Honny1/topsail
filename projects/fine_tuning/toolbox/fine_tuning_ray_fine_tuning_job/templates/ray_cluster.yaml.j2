# this resource (.spec) is currently used AS PART of the the RayJob,
# NOT directly.
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: {{ fine_tuning_ray_fine_tuning_job_name }}
  namespace: {{ fine_tuning_ray_fine_tuning_job_namespace }}
spec:
  headGroupSpec:
    enableIngress: false
    rayStartParams:
      block: 'true'
      dashboard-host: 0.0.0.0
      num-gpus: "{% if fine_tuning_ray_fine_tuning_job_gpu %}{{ fine_tuning_ray_fine_tuning_job_gpu }}{% else %}0{% endif %}"
      resources: '"{}"'
    serviceType: ClusterIP
    template: &head_template
      metadata:
        labels:
          ray-anti-affinity: "true"
      spec:
{% if fine_tuning_ray_fine_tuning_job_node_selector_key | length %}
        nodeSelector:
          {{ fine_tuning_ray_fine_tuning_job_node_selector_key }}: "{{ fine_tuning_ray_fine_tuning_job_node_selector_value }}"
{% endif %}
        containers:
        - name: ray-head
          ports:
          - containerPort: 6379
            name: gcs
            protocol: TCP
          - containerPort: 8265
            name: dashboard
            protocol: TCP
          - containerPort: 10001
            name: client
            protocol: TCP
          image: &head_image "{{ fine_tuning_ray_fine_tuning_job_container_image }}"

          env: &head_env
          - name: CONFIG_JSON_PATH
            value: /mnt/config/config.json
          resources: &head_resources
            requests: &head_request_block
{% if fine_tuning_ray_fine_tuning_job_gpu %}
              nvidia.com/gpu: "{{ fine_tuning_ray_fine_tuning_job_gpu }}"
{% endif %}
              memory: "{{ fine_tuning_ray_fine_tuning_job_memory }}Gi"
              cpu: "{{ fine_tuning_ray_fine_tuning_job_cpu }}"
{% if fine_tuning_ray_fine_tuning_job_request_equals_limits %}
            limits:  *head_request_block
{% else %}
            limits:
{% if fine_tuning_ray_fine_tuning_job_gpu %}
              nvidia.com/gpu: "{{ fine_tuning_ray_fine_tuning_job_gpu }}"
{% endif %}
              cpu: 20
{% endif %}

          volumeMounts: &head_volume_mounts
{% if fine_tuning_ray_fine_tuning_job_pvc_name %}
          - name: storage-volume
            mountPath: /mnt/storage
{% endif %}
          - name: app-volume
            mountPath: /mnt/app
          - name: entrypoint-volume
            mountPath: /mnt/entrypoint
          - name: config-volume
            mountPath: /mnt/config
          - name: output-volume
            mountPath: /mnt/output

        volumes: &head_volumes
{% if fine_tuning_ray_fine_tuning_job_pvc_name %}
        - name: storage-volume
          persistentVolumeClaim:
            claimName: {{ fine_tuning_ray_fine_tuning_job_pvc_name }}
{% endif %}
        - name: config-volume
          configMap:
            name: {{ job_name_safe }}-config
        - name: entrypoint-volume
          configMap:
            name: {{ job_name_safe }}-entrypoint
        - name: app-volume
          configMap:
            name: {{ job_name_safe }}-app
        - name: output-volume
          emptyDir: {}
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: ray-anti-affinity
                    operator: In
                    values:
                    - "true"
                topologyKey: "kubernetes.io/hostname"

  rayVersion: {{ fine_tuning_ray_fine_tuning_job_ray_version }}
  workerGroupSpecs:
  - groupName: {{ fine_tuning_ray_fine_tuning_job_name }}
    maxReplicas: {{ fine_tuning_ray_fine_tuning_job_pod_count -1 }}
    minReplicas: {{ fine_tuning_ray_fine_tuning_job_pod_count -1 }}
    rayStartParams:
      block: "true"
      num-gpus: "{% if fine_tuning_ray_fine_tuning_job_gpu %}{{ fine_tuning_ray_fine_tuning_job_gpu }}{% else %}0{% endif %}"
      resources: '"{}"'
    replicas: {{ fine_tuning_ray_fine_tuning_job_pod_count }}
    template: *head_template
