#! /usr/bin/env bash

set -o pipefail
set -o errexit
set -o nounset

THIS_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
cd ${THIS_DIR}/../..

source "${THIS_DIR}/../utils/logging.sh"

prepare_cluster_for_gpu_operator() {
    ./run_toolbox.py cluster capture_environment

    finalizers+=("collect_must_gather")

    if ! ./toolbox/wdm test library.nfd.has_nfd_labels --library; then
        _expected_fail "Checking if the cluster had NFD labels"

        if ./toolbox/wdm test library.nfd.has_nfd_in_operatorhub --library; then
            ./toolbox/wdm ensure library.nfd.has_nfd_from_operatorhub --library
        else
            _warning "NFD_deployed_from_master" "NFD was deployed from master (not available in OperatorHub)"

            # install the NFD Operator from sources
            repo="${1:-https://github.com/openshift/cluster-nfd-operator.git}"
            ref="${2:-master}"
            tag="ci-image"

            ./toolbox/wdm ensure has_nfd_from_master \
                          "--dependency-file=${THIS_DIR}/wdm/nfd_master.yaml" \
                          "--config=nfd_commit_ci_repo=$repo,nfd_commit_ci_ref=$ref,nfd_commit_ci_tag=$tag"
        fi
    fi

    GPU_NODE_CONFIG=--config=instance_type=g4dn.xlarge,instance_count=1
    if ! ./toolbox/wdm test library.gpu.has_gpu_nodes $GPU_NODE_CONFIG --library; then
        _expected_fail "Checking if the cluster had GPU nodes"

        ./toolbox/wdm ensure library.gpu.has_gpu_nodes $GPU_NODE_CONFIG --library
    fi
}

collect_must_gather() {
    run_in_sub_shell() {
        echo "Running the GPU Operator must-gather image ..."
        OPERATOR_IMAGE=$(oc get pods -A -lapp=gpu-operator -o=jsonpath='{.items[0].spec.containers[0].image}' 2> /dev/null || true)

        TMP_DIR="$(mktemp -d -t gpu-operator_XXXX)"

        if [[ "$OPERATOR_IMAGE" ]]; then
            echo "Operator image: $OPERATOR_IMAGE"

            oc adm must-gather --image="$OPERATOR_IMAGE" --dest-dir="${TMP_DIR}" &> /dev/null

            # ${TMP_DIR}/<image>/ should contain the file generated by
            # the must-gather script. If this is empty, there wasn't a
            # must-gather script in the image!
            if [[ "$(ls "${TMP_DIR}"/*/* 2>/dev/null | wc -l)" == 0 ]]; then
                echo "GPU Operator image failed to must-gather anything ..."
            else
                img_dirname=$(dirname "$(ls "${TMP_DIR}"/*/* | head -1)")
                mv "$img_dirname"/* $TMP_DIR
                rmdir "$img_dirname"

                # extract ARTIFACT_EXTRA_LOGS_DIR from 'source toolbox/_common.sh' without sourcing it directly
                export TOOLBOX_SCRIPT_NAME=toolbox/gpu-operator/must-gather.sh
                COMMON_SH=$(source toolbox/_common.sh;
                            echo "8<--8<--8<--";
                            # only evaluate these variables from _common.sh
                            env | egrep "(^ARTIFACT_EXTRA_LOGS_DIR=)"
                         )
                ENV=$(echo "$COMMON_SH" | sed '0,/8<--8<--8<--/d') # keep only what's after the 8<--
                eval $ENV

                echo "Copying must-gather results to $ARTIFACT_EXTRA_LOGS_DIR ..."
                cp -r "$TMP_DIR"/* "$ARTIFACT_EXTRA_LOGS_DIR"

                rmdir "$TMP_DIR"
            fi
        else
            echo "Failed to find the GPU Operator image ..."
        fi

        # Calling this until we're sure that the GPU Operator
        # must-gather image captures all the information we need
        echo "Running gpu_operator capture_deployment_state ..."
        ./run_toolbox.py gpu_operator capture_deployment_state > /dev/null || true

        echo "Running gpu_operator capture_deployment_state ... done."

        version=$(cat "$ARTIFACT_DIR"/*__gpu_operator__capture_deployment_state/gpu_operator.version 2> /dev/null || echo MISSING)
        echo "$version" > ${ARTIFACT_DIR}/operator.version

        if [[ "$version" != "MISSING" ]]; then
            echo "Operator versions collected."
        else
            echo "Failed to collect the operator version ..."
        fi
    }

    # run the function above in a subshell to avoid polluting the local `env`.
    typeset -fx run_in_sub_shell
    bash -c run_in_sub_shell
}

validate_gpu_operator_deployment() {
    ./run_toolbox.py gpu_operator wait_deployment
    ./run_toolbox.py gpu_operator run_gpu_burn
}

test_operatorhub() {
    OPERATOR_NAMESPACE="nvidia-gpu-operator"

    if [ "${1:-}" ]; then
        OPERATOR_VERSION="--version=$1"
    fi
    shift || true
    if [ "${1:-}" ]; then
        OPERATOR_CHANNEL="--channel=$1"
        if [[ "${OPERATOR_CHANNEL}" != *"1.9"* ]]; then
            OPERATOR_NAMESPACE="openshift-operators"
        fi
    fi
    shift || true

    prepare_cluster_for_gpu_operator "$@"

    ./run_toolbox.py gpu_operator deploy_from_operatorhub \
                     ${OPERATOR_CHANNEL:-} \
                     ${OPERATOR_VERSION:-} \
                     --namespace ${OPERATOR_NAMESPACE}

    validate_gpu_operator_deployment
}

if [ -z "${1:-}" ]; then
    echo "FATAL: $0 expects at least 1 argument ..."
    exit 1
fi

action="$1"
shift

if [[ "${action}" != "source" ]]; then
    set -x
fi

case ${action} in
    "test_operatorhub")
        test_operatorhub "$@"
        exit 0
        ;;
    "source")
        # file is being sourced by another script
        echo "INFO: GPU Operator CI entrypoint has been sourced"
        ;;
    *)
        echo "FATAL: Unknown action: ${action}" "$@"
        exit 1
        ;;
esac
