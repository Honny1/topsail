apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: {{ watsonx_serving_deploy_model_serving_runtime_name }}
  namespace: {{ watsonx_serving_deploy_model_namespace }}
spec:
  containers:
  - name: kserve-container
    command: [bash, -cex]
    args:
    - |
      echo "Starting Caikit-serving without logs ..."
      exec ./start-serving.sh &> /dev/null
    env:
    - name: RUNTIME_LOCAL_MODELS_DIR
      value: /mnt/models
{% if watsonx_serving_deploy_model_serving_runtime_resource_request['nvidia.com/gpu'] %}
    - name: NUM_GPUS
      value: "1"
{% endif %}
    - name: MODEL_NAME
      value: "{{ watsonx_serving_deploy_model_model_name }}"

    - name: TRANSFORMERS_CACHE
      value: /shared_model_storage/transformers_cache
    - name: HUGGINGFACE_HUB_CACHE
      value: /shared_model_storage/transformers_cache

{% for env_key, env_value in watsonx_serving_deploy_model_env_extra_values.items() %}
    - name: "{{ env_key }}"
      value: "{{ env_value }}"
{% endfor %}
    envFrom:
{% if watsonx_serving_deploy_model_secret_env_file_name is not none %}
    - secretRef:
        name: {{ watsonx_serving_deploy_model_serving_runtime_name }}-secret
{% endif %}

    image: {{ watsonx_serving_deploy_model_serving_runtime_image }}
    ports:
    # Note, KServe only allows a single port, this is the gRPC port. Subject to change in the future
    - containerPort: 8085
      name: h2c
      protocol: TCP
    resources:
      requests:
        cpu: "{{ watsonx_serving_deploy_model_serving_runtime_resource_request.cpu }}"
        memory: "{{ watsonx_serving_deploy_model_serving_runtime_resource_request.memory }}Gi"
{% if watsonx_serving_deploy_model_serving_runtime_resource_request['nvidia.com/gpu'] %}
        nvidia.com/gpu: "{{ watsonx_serving_deploy_model_serving_runtime_resource_request['nvidia.com/gpu'] }}"
{% endif %}
      limits:
        cpu: "{{ watsonx_serving_deploy_model_serving_runtime_resource_request.cpu }}"
        memory: "{{ watsonx_serving_deploy_model_serving_runtime_resource_request.memory }}Gi"
{% if watsonx_serving_deploy_model_serving_runtime_resource_request['nvidia.com/gpu'] %}
        nvidia.com/gpu: "{{ watsonx_serving_deploy_model_serving_runtime_resource_request['nvidia.com/gpu'] }}"
{% endif %}
    volumeMounts:
    - mountPath: /shared_model_storage/transformers_cache
      name: cache-volume
{% if watsonx_serving_deploy_model_runtime_config_file is not none %}
    - name: runtime-config
      subPath: runtime_config.yaml
      mountPath: "/caikit/config/caikit-tgis.yml"
{% endif %}
  volumes:
  - name:  cache-volume
    emptyDir:
      sizeLimit: 180Gi
{% if watsonx_serving_deploy_model_runtime_config_file is not none %}
  - name: runtime-config
    configMap:
      name: {{ watsonx_serving_deploy_model_serving_runtime_name }}-runtime-config
{% endif %}
  multiModel: false
  supportedModelFormats:
  # Note: this currently *only* supports caikit format models
  - autoSelect: true
    name: caikit
