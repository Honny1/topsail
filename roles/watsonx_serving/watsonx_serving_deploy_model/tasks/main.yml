---
- name: Create the src directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src"
    state: directory
    mode: '0755'

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts"
    state: directory
    mode: '0755'

# Cleanup the namespace
- name: Delete the InferenceServices
  command: oc delete InferenceServices,ServingRuntime --all -n {{ watsonx_serving_deploy_model_namespace }}

- name: Wait for the Pods to disappear
  command:
    oc get pods
       --no-headers
       -lcomponent=predictor
       -n {{ watsonx_serving_deploy_model_namespace }}
  register: ns_had_predictor_pods_cmd
  retries: 12
  delay: 10
  until: '"No resources found" in ns_had_predictor_pods_cmd.stderr'

# SMMR

- name: Delete the tracking timestamps
  command:
    oc delete cm -ltopsail.time-tracking -n {{ watsonx_serving_deploy_model_namespace }}

- name: Save timestamp
  shell: |
    NAME=start-deploy-model
    oc create configmap $NAME -n {{ watsonx_serving_deploy_model_namespace }}
    oc label cm/$NAME topsail.time-tracking=yes -n {{ watsonx_serving_deploy_model_namespace }}

- name: Prepare the SMMR
  block:
  - name: Patch the SMMR
    command: |
      oc patch smmr/default \
         -n istio-system \
         --type=json \
         -p="[{'op': 'add', 'path': '/spec/members/-', 'value': \"{{ watsonx_serving_deploy_model_namespace }}\"}]"
    register: patch_smmr_cmd
    failed_when: false

  - name: Check that the namespace is already registered
    when: patch_smmr_cmd.rc != 0
    shell:
      set -o pipefail;
      oc get smmr/default -n istio-system  -ojsonpath={.spec.members} | jq .[] -r
    register: smmr_members_cmd
    failed_when: watsonx_serving_deploy_model_namespace not in smmr_members_cmd.stdout_lines

  - name: Wait for the namespace to be registered
    shell:
      set -o pipefail;
      oc get smmr/default
         -n istio-system
         -ojsonpath={.status.configuredMembers}
         | jq '. | index("{{ watsonx_serving_deploy_model_namespace }}")'
    register: smmr_registered_namespace_cmd
    retries: 60
    delay: 10
    until: smmr_registered_namespace_cmd.stdout != "null"

  - name: Save timestamp
    shell: |
      NAME=smmr-registered-namespace
      oc create configmap $NAME -n {{ watsonx_serving_deploy_model_namespace }}
      oc label cm/$NAME topsail.time-tracking=yes -n {{ watsonx_serving_deploy_model_namespace }}

  always:
  - name: Capture the SMMR resource
    shell:
      oc get smmr/default
         -n istio-system
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/istio-system_smmr-default.yaml

# Secret

- name: Prepare the secret parameters
  shell: |
    set -o pipefail;
    set -e

    cat "{{ watsonx_serving_deploy_model_secret_env_file_name }}" | sha256sum > {{ artifact_extra_logs_dir }}/artifacts/secret_env_file.sha256sum

    secret_data=$(cat "{{ watsonx_serving_deploy_model_secret_env_file_name }}" \
     | yq '.["{{ watsonx_serving_deploy_model_secret_env_file_key }}"] | to_entries | map( { key: .key, value: .value|tostring|@base64 }) | from_entries')

    if [[ -z "$secret_data" ]]; then
      echo "Could not find key '{{ watsonx_serving_deploy_model_secret_env_file_key }}' in file {{ watsonx_serving_deploy_model_secret_env_file_name }}. Aborting."
      exit 1
    fi
    oc create secret generic {{ watsonx_serving_deploy_model_serving_runtime_name }}-secret \
       -n {{ watsonx_serving_deploy_model_namespace }} \
       -ojson \
       --dry-run=client \
       | jq --argjson data "$secret_data" '.data = $data' \
       | oc apply -f-

    oc describe secret/{{ watsonx_serving_deploy_model_serving_runtime_name }}-secret \
       -n {{ watsonx_serving_deploy_model_namespace }} \
       > {{ artifact_extra_logs_dir }}/artifacts/env-secrets.desc

  when: watsonx_serving_deploy_model_secret_env_file_name != None

# Serving Runtime

- name: Prepare the runtime config file configmap
  shell:
    set -o pipefail;

    oc create cm {{ watsonx_serving_deploy_model_serving_runtime_name }}-runtime-config
       -n {{ watsonx_serving_deploy_model_namespace }}
       --from-file=runtime_config.yaml="{{ watsonx_serving_deploy_model_runtime_config_file }}"
       -oyaml
       --dry-run=client
       | tee "{{ artifact_extra_logs_dir }}/src/caikit_runtime_config.yaml"
       | oc apply -f-
  when: watsonx_serving_deploy_model_runtime_config_file != None

- name: Prepare the ServingRuntime template
  template:
    src: "{{ serving_runtime_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"
    mode: '0400'

- name: Create the ServingRuntime
  command:
    oc apply -f "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"

# Inference Service

- name: Prepare the InferenceService template
  template:
    src: "{{ inference_service_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"
    mode: '0400'

- name: Create the InferenceService
  command:
    oc apply -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"

- name: Prepare the InferenceService
  block:
  - name: Wait for the InferenceService Pod to appear
    command:
      oc get pod
      -oname
      -lserving.kserve.io/inferenceservice={{ watsonx_serving_deploy_model_inference_service_name }}
      -n {{ watsonx_serving_deploy_model_namespace }}
    register: inference_service_pod_name
    # wait 5 minutes
    retries: 30
    delay: 10
    until: inference_service_pod_name.stdout | length > 0

  - name: Wait for the InferenceService Pod to be scheduled
    command:
      oc get pod
      -ojsonpath={.items[0].spec.nodeName}
      -lserving.kserve.io/inferenceservice={{ watsonx_serving_deploy_model_inference_service_name }}
      -n {{ watsonx_serving_deploy_model_namespace }}
    register: inference_service_pod_nodename
    # wait 1 minutes
    retries: 6
    delay: 10
    until: inference_service_pod_nodename.stdout | length > 0

  - name: Wait for the InferenceService to be loaded
    shell:
      set -o pipefail;
      oc get -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"
         -ojsonpath={.status.modelStatus.states.targetModelState}
    register: inference_service_state_cmd
    # wait 15 minutes
    retries: 90
    delay: 10
    until: inference_service_state_cmd.stdout == "Loaded"

  - name: Capture the state of the InferenceService Pod resource
    shell:
      oc get pod
         -lserving.kserve.io/inferenceservice={{ watsonx_serving_deploy_model_inference_service_name }}
         -owide

  - name: Save timestamp
    shell: |
      NAME=inference-service-loaded
      oc create configmap $NAME -n {{ watsonx_serving_deploy_model_namespace }}
      oc label cm/$NAME topsail.time-tracking=yes -n {{ watsonx_serving_deploy_model_namespace }}

  - name: Ensure that grpcurl is available
    command: which grpcurl

  - name: Validate the model deployment
    when: watsonx_serving_deploy_model_query_data is not none
    shell: |
      set -o pipefail
      set -e

      GRPCURL_DATA=$(echo "{{ watsonx_serving_deploy_model_query_data }}" | sed "s/'/\"/g")
      HOSTNAME=$(oc get isvc -ojsonpath={.items[0].status.components.predictor.url} -n {{ watsonx_serving_deploy_model_namespace }} | cut -d/ -f3)

      grpcurl -insecure \
              -d "$GRPCURL_DATA" \
              -H "mm-model-id: {{ watsonx_serving_deploy_model_model_id }}" \
              "$HOSTNAME:443" \
              caikit.runtime.Nlp.NlpService/TextGenerationTaskPredict \
    register: grpc_validation_cmd
    failed_when: '"ERROR" in grpc_validation_cmd.stderr'

  always:
  - name: Capture the state of the InferenceService Pod resource
    shell:
      oc get pod
         -lserving.kserve.io/inferenceservice={{ watsonx_serving_deploy_model_inference_service_name }}
         -oyaml
         -n {{ watsonx_serving_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.yaml;
      oc get pod
         -lserving.kserve.io/inferenceservice={{ watsonx_serving_deploy_model_inference_service_name }}
         -owide
         -n {{ watsonx_serving_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.status;
      oc describe pod
         -lserving.kserve.io/inferenceservice={{ watsonx_serving_deploy_model_inference_service_name }}
         -n {{ watsonx_serving_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.desc
    ignore_errors: true

  - name: Capture the state of the InferenceService resource
    shell:
      oc get -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/inference_service.yaml

  - name: Capture the state of the ServingRuntime resource
    shell:
      oc get -f "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/serving_runtime.yaml

  - name: Save the timestamp configmaps
    shell:
      oc get cm -oyaml
         -ltopsail.time-tracking=yes
         -n {{ watsonx_serving_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/time_tracking_cm.yaml
