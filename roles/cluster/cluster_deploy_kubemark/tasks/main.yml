---
- name: Create the source directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src"
    state: directory
    mode: '0755'

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts"
    state: directory
    mode: '0755'

- name: Define versions and properties
  set_fact:
    clusterctl_version: v1.5.1
    capi_kubemark_version: v0.6.0
    capi_operator_version: main
    kubemark_worker_machine_count: 4
    capi_namespace: openshift-cluster-api

- name: Get the name of the cluster
  command:
    oc get infrastructure cluster -o jsonpath="{.status.infrastructureName}"
  register: cluster_name_cmd

- name: Get the K8s version of the cluster
  shell:
    set -o pipefail;
    oc version -ojson | jq -r .serverVersion.gitVersion | cut -d+ -f1 | cut -b2-
  register: k8s_version_cmd

- name: Save the cluster name
  set_fact:
    cluster_name: "{{ cluster_name_cmd.stdout }}"
    k8s_version: "{{ k8s_version_cmd.stdout }}"

# ---

- name: Enable TechPreview
  shell:
    set -o pipefail;
    oc get FeatureGate/cluster -ojson
       | jq '.spec.featureSet = "TechPreviewNoUpgrade"'
       | tee {{ artifact_extra_logs_dir }}/artifacts/tpnu.json
       | oc apply -f-

- name: Wait for TPNU to create the openshift-cluster-api namespace
  shell:
    oc get ns openshift-cluster-api -oname 2>/dev/null
  register: has_namespace_cmd
  until: has_namespace_cmd.rc == 0
  retries: 30
  delay: 30

- name: Wait for the coreprovider CRD to appear
  command:
    oc get crd/coreproviders.operator.cluster.x-k8s.io
       -oname
       --ignore-not-found
  register: has_crd_cmd
  until: has_crd_cmd.stdout | length > 0
  retries: 60
  delay: 20

- name: Wait for the cluster-api to be installed
  shell:
    oc get coreprovider/cluster-api
       -n {{ capi_namespace }}
       -ojsonpath={.status.installedVersion}
       --ignore-not-found
  register: has_cluster_api_cmd
  until: has_cluster_api_cmd.stdout | length > 0
  retries: 30
  delay: 10

- name: Wait for the aws provider to be installed
  shell:
    oc get infrastructureproviders/aws
    -n {{ capi_namespace }}
    -ojsonpath={.status.installedVersion}
    --ignore-not-found
  register: has_aws_api_cmd
  until: has_aws_api_cmd.stdout | length > 0
  retries: 30
  delay: 10

- name: Wait for the worker nodes to be upgraded
  shell:
    set -o pipefail;
    oc get mcp -lpools.operator.machineconfiguration.openshift.io/worker
       -ojsonpath='{range .items[*]}{.metadata.name}{" ="}{.status.unavailableMachineCount}{"=\n"}{end}'
       | grep -v "=0="
  register: has_unavailable_machines
  until: not has_unavailable_machines.rc != 1
  failed_when: has_unavailable_machines.rc != 1
  retries: 90
  delay: 30

# ---

- name: Install clusterctl
  shell: |
    curl -sSf --silent -L https://github.com/kubernetes-sigs/cluster-api/releases/download/{{ clusterctl_version }}/clusterctl-linux-amd64 -o /tmp/clusterctl
    chmod +x /tmp/clusterctl

- name: Prepare the config file
  shell: |
    cat <<EOF > {{ artifact_extra_logs_dir }}/clusterctl.yaml
    providers:
    - name: "kubemark"
      url: "https://github.com/kubernetes-sigs/cluster-api-provider-kubemark/releases/{{ capi_kubemark_version }}/infrastructure-components.yaml"
      type: "InfrastructureProvider"
    EOF

- name: Initialize kubemark provider
  shell:
    set -o pipefail;
    set -e;

    /tmp/clusterctl init
       -v10
       --target-namespace {{ capi_namespace }}
       --config {{ artifact_extra_logs_dir }}/clusterctl.yaml;

    /tmp/clusterctl generate provider
       -v10
       --target-namespace {{ capi_namespace }}
       --bootstrap kubeadm
       --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
       | tee "{{ artifact_extra_logs_dir }}/src/stack_bs_kubeadm.yaml"
       | oc apply -f-;

    /tmp/clusterctl generate provider
       -v10
       --target-namespace {{ capi_namespace }}
       --control-plane kubeadm
       --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
       | tee "{{ artifact_extra_logs_dir }}/src/stack_cp_kubemark.yaml"
       | oc apply -f-;

   /tmp/clusterctl generate provider
       -v10
       --target-namespace {{ capi_namespace }}
       --infrastructure kubemark
       --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
       | tee "{{ artifact_extra_logs_dir }}/src/stack_infra_kubemark.yaml"
       | oc apply -f-;

- name: Workaround | Give more memory to CAPK (was dying because of OOM)
  shell: |
    oc set resources deploy/capk-controller-manager --limits="cpu=0.5,memory=200Mi" -n {{ capi_namespace }}

# ---

# Beginning of the work arounds

# Workaround
- name: Apply the workarounds (controller dying because of OOM)
  shell: |
    oc set resources deploy/capk-controller-manager --limits="cpu=0.5,memory=200Mi" -n {{ capi_namespace }}

- name: Apply the workarounds (Pods not starting because of missing privileges)
  shell: |
    oc adm policy add-scc-to-user privileged -z capi-kubeadm-bootstrap-manager -n {{ capi_namespace }}
    oc adm policy add-scc-to-user privileged -z capi-kubeadm-control-plane-manager -n {{ capi_namespace }}

    oc delete replicaset --all -n {{ capi_namespace }}

# ---

# workaround (invalid AWS credentials)
- name: Wait for the capAWS secret to be created
  shell:
    oc get secret capa-manager-bootstrap-credentials
       -oname
       -n {{ capi_namespace }}
  register: has_secret_cmd
  until: has_secret_cmd.rc == 0
  retries: 60
  delay: 10

# - name: Create the capAWS secret
#   shell:
#     set -o pipefail;
#     set -e

#     AWS_CREDS_FILE=$PSAP_ODS_SECRET_PATH/.awscred;
#     key_id=$(cat "$AWS_CREDS_FILE" | grep aws_access_key_id | head -1 | cut -d" " -f3- | sed 's/ //g');
#     access_key=$(cat "$AWS_CREDS_FILE" | grep aws_secret_access_key | head -1 | cut -d" " -f3-);
#     oc create secret generic capa-manager-bootstrap-credentials
#        --from-literal=credentials="$(echo -n "[default]\naws_access_key_id = $key_id\naws_secret_access_key=$access_key\n")"
#        --from-literal=aws_access_key_id="$key_id"
#        --from-literal=aws_secret_access_key="$access_key"
#        -n "{{ capi_namespace }}"
#        -oyaml
#        --dry-run=client
#      | oc replace -f-

# ---

- name: Wait for the CAPI deployments to be ready
  shell:
    set -o pipefail;
    oc get deploy -n "{{ capi_namespace }}" -ojsonpath='{range .items[*]}{.metadata.name}{" ="}{.status.unavailableReplicas}{"=\n"}{end}' | grep -v "=="
  register: has_unavailable_deployment
  until: not has_unavailable_deployment.rc != 1
  failed_when: has_unavailable_deployment.rc != 1
  retries: 90
  delay: 30

# ---

- name: Create the CAPI AWSCluster
  shell: |
    set -o pipefail;
    set -e

    AWS_REGION=$(oc get machineset.machine.openshift.io -n openshift-machine-api -o jsonpath="{.items[0].spec.template.spec.providerSpec.value.placement.region}")

    cat <<EOF | tee "{{ artifact_extra_logs_dir }}/src/capi_aws_cluster.yaml" | oc apply -f-
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AWSCluster
    metadata:
      name: {{ cluster_name }}
      namespace: {{ capi_namespace }}
      annotations:
        cluster.x-k8s.io/managed-by: ""
    spec:
      region: ${AWS_REGION}
    EOF


- name: Get the CAPI Core cluster template
  shell: |
    set -o pipefail

    cat <<EOF | tee "{{ artifact_extra_logs_dir }}/src/capi_core_cluster.yaml" | oc apply -f-
    apiVersion: cluster.x-k8s.io/v1beta1
    kind: Cluster
    metadata:
      name: {{ cluster_name }}
      namespace: {{ capi_namespace }}
    spec:
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSCluster
        name: {{ cluster_name }}
        namespace: {{ capi_namespace }}
    EOF

# ---

- name: Wait for the AWCluster to turn ready and capture artifacts
  block:
  - name: Wait for the AWCluster to turn ready
    command:
      oc get awscluster/"{{ cluster_name }}"
         -n {{ capi_namespace }}
         -ojsonpath={.status.ready}
    register: aws_cluster_ready_cmd
    until: aws_cluster_ready_cmd.stdout == "true"
    retries: 30
    delay: 20

  # ---

  - name: Create the kubemark cluster resources
    shell:
      set -o pipefail;
      /tmp/clusterctl generate cluster "{{ cluster_name }}"
          -v10
          --infrastructure kubemark:{{ capi_kubemark_version }}
          --kubernetes-version {{ k8s_version }}
          --worker-machine-count={{ kubemark_worker_machine_count }}
          --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
        | yq -Y '.metadata.namespace = "{{ capi_namespace }}"'
        | yq -Y 'select(.kind =="MachineDeployment") .spec.template.spec.bootstrap.dataSecretName = "worker-user-data"'
        | yq -Y 'select(.kind =="KubemarkMachineTemplate") .spec.template.spec.extraMounts[0].hostPath = "/run/crio/crio.sock"'
        | tee "{{ artifact_extra_logs_dir }}/src/kubemark_cluster.yaml"
        | oc apply -f-


  - name: workaround | Create the cluster CA secret
    shell:
      set -o pipefail;
      set -e;

      oc create secret generic "{{ cluster_name }}-ca"
         -n {{ capi_namespace }}
         --from-literal=tls.crt="$(oc get secret/capi-webhook-service-cert -ojsonpath="{.data.tls\.crt}" -n {{ capi_namespace }} | base64 -d)"
         --from-literal=tls.key="$(oc get secret/capi-webhook-service-cert -ojsonpath="{.data.tls\.key}" -n {{ capi_namespace }} | base64 -d)"
         --dry-run=client
         -oyaml
         | oc apply -f-

  - name: Wait for the hollow node Pods to be created
    command:
      oc get pods
         -lapp=hollow-node
         -n {{ capi_namespace }}
    register: has_hollow_node_pods
    until: has_hollow_node_pods.stdout | length > 0
    retries: 30
    delay: 10

  - name: Wait for the hollow node Pods to start running
    shell:
      set -o pipefail;

      oc get pods
         -lapp=hollow-node
         -n {{ capi_namespace }}
         | grep Running

    register: has_hollow_node_pods
    until: has_hollow_node_pods.stdout | length > 0
    retries: 3
    delay: 10
    ignore_errors: true

  always:
  - name: Capture the state of the cluster
    shell:
      set -o pipefail;
      set -e;

      /tmp/clusterctl describe cluster "{{ cluster_name }}"
          --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
          -n {{ capi_namespace }}
          | tee "{{ artifact_extra_logs_dir }}/artifacts/cluster.desc";

      oc get cluster/"{{ cluster_name }}"
         -oyaml
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/cluster.yaml";

      oc get awscluster/"{{ cluster_name }}"
         -oyaml
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/aws_cluster.yaml";

      oc get machinedeployments.cluster.x-k8s.io
         -oyaml
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/machinedeployments.yaml";

  - name: Capture the state of the pods
    shell:
      oc get pods
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/capi_pods.status"
