---
- name: Create the source directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src"
    state: directory
    mode: '0755'

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts"
    state: directory
    mode: '0755'

- name: Define versions and properties
  set_fact:
    clusterctl_version: v1.5.1
    capi_kubemark_version: v0.6.0
    capi_operator_version: main
    kubemark_worker_machine_count: 4
    capi_namespace: openshift-cluster-api

    kubemark_node_resource: {"cpu": "1", "memory": "4G", "nvidia.com/gpu": "1"}

    kubemark_node_labels: kubemark=true only-workload-pods=yes node-role.kubernetes.io/workload=
    kubemark_node_taint_key: only-workload-pods
    kubemark_node_taint_value: "yes"
    kubemark_node_taint_effect: NoSchedule

- name: Get the name of the cluster
  command:
    oc get infrastructure cluster -o jsonpath="{.status.infrastructureName}"
  register: cluster_name_cmd

- name: Get the K8s version of the cluster
  shell:
    set -o pipefail;
    oc version -ojson | jq -r .serverVersion.gitVersion | cut -d+ -f1 | cut -b2-
  register: k8s_version_cmd

- name: Save the cluster name
  set_fact:
    cluster_name: "{{ cluster_name_cmd.stdout }}"
    k8s_version: "{{ k8s_version_cmd.stdout }}"

# ---

- name: Enable TechPreview
  shell:
    set -o pipefail;
    oc get FeatureGate/cluster -ojson
       | jq '.spec.featureSet = "TechPreviewNoUpgrade"'
       | tee {{ artifact_extra_logs_dir }}/artifacts/tpnu.json
       | oc apply -f-

- name: Wait for TPNU to create the openshift-cluster-api namespace
  shell:
    oc get ns openshift-cluster-api -oname 2>/dev/null
  register: has_namespace_cmd
  until: has_namespace_cmd.rc == 0
  retries: 30
  delay: 30

- name: Wait for the coreprovider CRD to appear
  command:
    oc get crd/coreproviders.operator.cluster.x-k8s.io
       -oname
       --ignore-not-found
  register: has_crd_cmd
  until: has_crd_cmd.stdout | length > 0
  retries: 60
  delay: 20

- name: Wait for the cluster-api to be installed
  shell:
    oc get coreprovider/cluster-api
       -n {{ capi_namespace }}
       -ojsonpath={.status.installedVersion}
       --ignore-not-found
  register: has_cluster_api_cmd
  until: has_cluster_api_cmd.stdout | length > 0
  retries: 30
  delay: 10

- name: Wait for the aws provider to be installed
  shell:
    oc get infrastructureproviders/aws
    -n {{ capi_namespace }}
    -ojsonpath={.status.installedVersion}
    --ignore-not-found
  register: has_aws_api_cmd
  until: has_aws_api_cmd.stdout | length > 0
  retries: 30
  delay: 10

- name: Wait for the worker nodes to be upgraded
  shell:
    set -o pipefail;
    oc get mcp -lpools.operator.machineconfiguration.openshift.io/worker
       -ojsonpath='{range .items[*]}{.metadata.name}{" ="}{.status.unavailableMachineCount}{"=\n"}{end}'
       | grep -v "=0="
  register: has_unavailable_machines
  until: not has_unavailable_machines.rc != 1
  failed_when: has_unavailable_machines.rc != 1
  retries: 90
  delay: 30

# ---

- name: Install clusterctl
  shell: |
    curl -sSf --silent -L https://github.com/kubernetes-sigs/cluster-api/releases/download/{{ clusterctl_version }}/clusterctl-linux-amd64 -o /tmp/clusterctl
    chmod +x /tmp/clusterctl

- name: Prepare the config file
  shell: |
    cat <<EOF > {{ artifact_extra_logs_dir }}/clusterctl.yaml
    providers:
    - name: "kubemark"
      url: "https://github.com/kubernetes-sigs/cluster-api-provider-kubemark/releases/{{ capi_kubemark_version }}/infrastructure-components.yaml"
      type: "InfrastructureProvider"
    EOF

- name: Initialize kubemark provider
  shell:
    set -o pipefail;
    set -e;

    /tmp/clusterctl init
       -v10
       --target-namespace {{ capi_namespace }}
       --config {{ artifact_extra_logs_dir }}/clusterctl.yaml;

    /tmp/clusterctl generate provider
       -v10
       --target-namespace {{ capi_namespace }}
       --bootstrap kubeadm
       --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
       | tee "{{ artifact_extra_logs_dir }}/src/stack_bs_kubeadm.yaml"
       | oc apply -f-;

    /tmp/clusterctl generate provider
       -v10
       --target-namespace {{ capi_namespace }}
       --control-plane kubeadm
       --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
       | tee "{{ artifact_extra_logs_dir }}/src/stack_cp_kubemark.yaml"
       | oc apply -f-;

   /tmp/clusterctl generate provider
       -v10
       --target-namespace {{ capi_namespace }}
       --infrastructure kubemark
       --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
       | tee "{{ artifact_extra_logs_dir }}/src/stack_infra_kubemark.yaml"
       | oc apply -f-;

- name: Workaround | Give more memory to CAPK (was dying because of OOM)
  shell: |
    oc set resources deploy/capk-controller-manager --limits="cpu=0.5,memory=200Mi" -n {{ capi_namespace }}

# ---

# Beginning of the work arounds

# Workaround
- name: Apply the workarounds (controller dying because of OOM)
  shell: |
    oc set resources deploy/capk-controller-manager --limits="cpu=0.5,memory=200Mi" -n {{ capi_namespace }}

- name: Apply the workarounds (Pods not starting because of missing privileges)
  shell: |
    oc adm policy add-scc-to-user privileged -z capi-kubeadm-bootstrap-manager -n {{ capi_namespace }}
    oc adm policy add-scc-to-user privileged -z capi-kubeadm-control-plane-manager -n {{ capi_namespace }}

    oc delete replicaset --all -n {{ capi_namespace }}

# ---

- name: Wait for the CAPI deployments to be ready
  shell:
    set -o pipefail;
    oc get deploy -n "{{ capi_namespace }}" -ojsonpath='{range .items[*]}{.metadata.name}{" ="}{.status.unavailableReplicas}{"=\n"}{end}' | grep -v "=="
  register: has_unavailable_deployment
  until: not has_unavailable_deployment.rc != 1
  failed_when: has_unavailable_deployment.rc != 1
  retries: 90
  delay: 30

# ---

- name: Create the CAPI AWSCluster
  shell: |
    set -o pipefail;
    set -e

    AWS_REGION=$(oc get machineset.machine.openshift.io -n openshift-machine-api -o jsonpath="{.items[0].spec.template.spec.providerSpec.value.placement.region}")

    cat <<EOF | tee "{{ artifact_extra_logs_dir }}/src/capi_aws_cluster.yaml" | oc apply -f-
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AWSCluster
    metadata:
      name: {{ cluster_name }}
      namespace: {{ capi_namespace }}
      annotations:
        cluster.x-k8s.io/managed-by: ""
    spec:
      region: ${AWS_REGION}
    EOF


- name: Create the CAPI Core cluster
  shell: |
    set -o pipefail

    cat <<EOF | tee "{{ artifact_extra_logs_dir }}/src/capi_core_cluster.yaml" | oc apply -f-
    apiVersion: cluster.x-k8s.io/v1beta1
    kind: Cluster
    metadata:
      name: {{ cluster_name }}
      namespace: {{ capi_namespace }}
    spec:
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSCluster
        name: {{ cluster_name }}
        namespace: {{ capi_namespace }}
    EOF

- name: Wait for the AWCluster to turn ready
  command:
    oc get awscluster/"{{ cluster_name }}"
       -n {{ capi_namespace }}
       -ojsonpath={.status.ready}
  register: aws_cluster_ready_cmd
  until: aws_cluster_ready_cmd.stdout == "true"
  retries: 30
  delay: 20

# ---

- name: Workaround | Create the cluster CA secret and clean it up
  block:
  - name: Create the RSA key
    command:
      openssl req -x509 -subj "/CN=Kubernetes API" -new -newkey rsa:2048 -nodes -keyout /tmp/tls.key -sha256 -days 3650 -out /tmp/tls.crt

  - name: Create the cluster CA secret
    shell:
      set -o pipefail;

      oc create secret tls "{{ cluster_name }}-ca"
         -n {{ capi_namespace }}
         --cert=/tmp/tls.crt
         --key=/tmp/tls.key
         --dry-run=client
         -oyaml
         | oc apply -f-

  always:
  - name: Clean up the secret files
    shell: |
      rm -f /tmp/tls.key /tmp/tls.crt

# ---

- name: Create and await the Kubemark hollow nodes
  block:
  - name: Create the kubemark cluster resources
    shell:
      set -o pipefail;
      /tmp/clusterctl generate cluster "{{ cluster_name }}"
          -v10
          --infrastructure kubemark:{{ capi_kubemark_version }}
          --kubernetes-version {{ k8s_version }}
          --worker-machine-count={{ kubemark_worker_machine_count }}
          --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
        | yq -Y '.metadata.namespace = "{{ capi_namespace }}"'
        | yq -Y 'select(.kind =="MachineDeployment") .spec.template.spec.bootstrap.dataSecretName = "worker-user-data"'
        | yq -Y 'select(.kind =="KubemarkMachineTemplate") .spec.template.spec.extraMounts[0].hostPath = "/run/crio/crio.sock"'
        | yq -Y 'select(.kind =="KubemarkMachineTemplate") .spec.template.spec.kubemarkOptions.extendedResources = {{ kubemark_node_resource }}'

        | tee "{{ artifact_extra_logs_dir }}/src/kubemark_cluster.yaml"
        | oc apply -f-

  - name: Wait for the hollow node Pods to be created
    command:
      oc get pods
         -lapp=hollow-node
         -n {{ capi_namespace }}
    register: has_hollow_node_pods
    until: has_hollow_node_pods.stdout | length > 0
    retries: 30
    delay: 10

  - name: Wait for the hollow node Pods to start running
    shell:
      set -o pipefail;
      oc get pods
         -lapp=hollow-node
         -n {{ capi_namespace }}
         | grep Running

    register: has_hollow_node_pods
    until: has_hollow_node_pods.stdout | length > 0
    retries: 3
    delay: 10

  # this step is expected to fail at the moment
  - name: Wait for the kubemark nodes to appear | Failure expected
    shell:
      set -o pipefail;

      oc get node | grep kubemark
    register: has_hollow_node
    until: has_hollow_node.stdout | length == kubemark_worker_machine_count
    retries: 3
    delay: 10

  - name: Indicate that Kubemark test passed
    shell:
      touch "{{ artifact_extra_logs_dir }}/CAPK_WORKED";
      touch "{{ artifact_extra_logs_dir }}/../CAPK_WORKED";

  - name: Abort if we reach this step, this is not expected so far
    meta: end_play

  rescue:
  - name: Workaround | Create a working kubeconfig secret
    shell:
      set -o pipefail;
      oc create secret generic {{ cluster_name }}-kubemark-kubeconfig
         --from-file=kubeconfig=$KUBECONFIG
         --dry-run=client -oyaml
         -n {{ capi_namespace }}
         | oc apply -f-

  - name: Workaround | Take the name of the Kubemark Pods
    shell:
      set -o pipefail;
      oc get pods -lapp=hollow-node -oname -n {{ capi_namespace }}
         | cut -d/ -f2
         | grep -v 'workaround'
    register: kubemark_pod_names_cmd

  - name: Workaround | Strip the Pod defininition
    shell:
      set -o pipefail;
      oc get pod/{{ kubemark_pod_name }} -n {{ capi_namespace }} -oyaml
         | yq -Y 'del(.status) | del(.spec.volumes[] | select(.name | startswith("kube-api-access-"))) | del(.spec.containers[0].volumeMounts[] | select(.name | startswith("kube-api-access-"))) | del(.spec.nodeName) | del(.metadata.uid) | del(.metadata.creationTimestamp) | del(.metadata.resourceVersion) | del(.metadata.annotations) | del(.spec.tolerations)'
         | yq -Y '.metadata.name += "-workaround"' | yq -Y '.spec.volumes[0].secret.secretName = "{{ cluster_name }}-kubemark-kubeconfig"'
         | tee "{{ artifact_extra_logs_dir }}/src/kubemark_pod.yaml"
         | oc apply -f-
    loop: "{{ kubemark_pod_names_cmd.stdout_lines }}"
    loop_control:
      loop_var: kubemark_pod_name

  - name: Wait for the kubemark node to appear
    shell:
      oc get node/{{ kubemark_pod_name }} --ignore-not-found
    register: has_hollow_node
    until: has_hollow_node.stdout | length > 0
    retries: 3
    delay: 10
    loop: "{{ kubemark_pod_names_cmd.stdout_lines }}"
    loop_control:
      loop_var: kubemark_pod_name

  always:
  - name: Capture the state of the cluster
    shell:
      set -o pipefail;
      set -e;

      /tmp/clusterctl describe cluster "{{ cluster_name }}"
          --config {{ artifact_extra_logs_dir }}/clusterctl.yaml
          -n {{ capi_namespace }}
          | tee "{{ artifact_extra_logs_dir }}/artifacts/cluster.desc";

      oc get cluster/"{{ cluster_name }}"
         -oyaml
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/cluster.yaml";

      oc get awscluster/"{{ cluster_name }}"
         -oyaml
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/aws_cluster.yaml";

      oc get machinedeployments.cluster.x-k8s.io
         -oyaml
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/machinedeployments.yaml";

      oc get machines.cluster.x-k8s.io
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/machines.status";

  - name: Capture the status of the CAPI pods
    shell:
      oc get pods
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/capi_pods.status"

  - name: Capture the description of the hollow node pods
    shell:
      oc describe pods
         -lapp=hollow-node
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/hollow_node_pods.desc";

      oc get pods
         -lapp=hollow-node
         -n {{ capi_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/hollow_node_pods.status"

- name: Take the name of the Kubemark Nodes
  shell:
    set -o pipefail;
    oc get nodes -oname
      | grep 'kubemark'
      | cut -d/ -f2
  register: kubemark_node_names_cmd

- name: Taint the kubemark nodes
  shell:
    oc adm taint node --overwrite
      {{ kubemark_node_names_cmd.stdout_lines | join(' ')}}
      {{ kubemark_node_taint_key }}={{ kubemark_node_taints_value }}:{{ kubemark_node_taints_effect }}
  when: kubemark_node_taints and kubemark_node_taints | length > 0

- name: Label the kubemark nodes
  shell:
    oc label node --overwrite
       {{ kubemark_node_names_cmd.stdout_lines | join(' ')}}
       {{ kubemark_node_labels }}
  when: kubemark_node_labels and kubemark_node_labels | length > 0

# ---

- name: Create test Pods on the Kubemark nodes
  shell: |
    set -o pipefail;

    cat <<EOF | tee "{{ artifact_extra_logs_dir }}/src/kubemark_dummy_pod-{{ kubemark_node_name }}.yaml" | oc apply -f- -n {{ capi_namespace }}
    apiVersion: v1
    kind: Pod
    metadata:
      name: test-pod-{{ kubemark_node_name }}
      labels:
        kubemark: "true"
    spec:
      containers:
      - name: cnt
        image: registry.access.redhat.com/ubi9/ubi
        command: [ "do", "nothing"]
      nodeSelector:
         kubernetes.io/hostname: {{ kubemark_node_name }}
      tolerations:
      - key: {{ kubemark_node_taint_key }}
        operator: "Exists"
        effect: {{ kubemark_node_taint_effect }}

    EOF
  loop: "{{ kubemark_node_names_cmd.stdout_lines }}"
  loop_control:
    loop_var: kubemark_node_name

- name: Wait for the Pods to start running
  shell:
    set -o pipefail;

    oc get pod/test-pod-{{ kubemark_node_name }} -n {{ capi_namespace }} | grep Running
  register: dummy_pod_running
  until: dummy_pod_running.rc == 0
  retries: 3
  delay: 10
  loop: "{{ kubemark_node_names_cmd.stdout_lines }}"
  loop_control:
    loop_var: kubemark_node_name

- name: Capture the description of the Kubemark nodes
  shell:
    oc describe node/{{ kubemark_node_name }} > "{{ artifact_extra_logs_dir }}/artifacts/kubemark_node_{{ kubemark_node_name }}.descr";
    oc logs pod/{{ kubemark_node_name }} -n {{ capi_namespace }}  > "{{ artifact_extra_logs_dir }}/artifacts/kubemark_pod_{{ kubemark_node_name }}.log";
  loop: "{{ kubemark_node_names_cmd.stdout_lines }}"
  loop_control:
    loop_var: kubemark_node_name
